{
  "fields": {
    "version_num": 0,
    "title": "Markov decision process (MDP)",
    "pointers": "* There are several methods for finding the optimal policy for an MDP:\n**  [Value iteration](value_iteration)  is an application of dynamic programming that recursively computes the value function. It does not scale well as it has a complexity of O(S^2A) for S states and A actions.\n**  [Policy iteration](policy_iteration)  is similar to value iteration, but it alternates between determining the value function given a fixed policy and choosing a policy given a fixed value function. It tends to converge much quicker than value iteration.\n* The  [Bellman equations](bellman_equations)  characterize the optimal values for MDPs and are ubiquitous in reinforcement learning.",
    "tags": ["pgm", "reinforcementlea"],
    "learn_time": 1.2536806942788978,
    "summary": "A Markov Decision Process (MDP) is a mathematical framework for handling search/planning problems where the outcome of actions are uncertain (non-deterministic). MDPs aim to maximize the expected utility (minimize the expected loss) throughout the search/planning.\n",
    "last_mod": "2014-08-13T09:59:02.294Z",
    "tag": "markov_decision_process",
    "exercises": null,
    "is_shortcut": false,
    "software": null
  },
  "model": "graph.concept",
  "pk": "q3j5cjx4"
}
