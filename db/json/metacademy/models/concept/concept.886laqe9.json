{
  "fields": {
    "version_num": 0,
    "title": "loopy BP as variational inference",
    "pointers": "* Loopy BP is  [guaranteed to converge](gaussian_loopy_bp)  to the correct mean for Gaussian graphical models.\n* Loopy BP is  [guaranteed to converge](loopy_bp_single_loop)  for  a graph with a single loop. \n*  [Tree-reweighted belief propagation](tree_reweighted_belief_propagation)  is an algorithm inspired by the same ideas, but where the approximation to KL divergence is convex and gives an upper bound on the partition function. \n* Some other inference algorithms based on variational principles:\n**  [expectation propagation](expectation_propagation) , which approximates BP messages in terms of expectations \n**  [mean field approximation](mean_field_approximation) , where different variables are approximated as independent in the posterior \n**  [variational Bayes](variational_bayes) , a general framework for posterior inference in Bayesian models ",
    "tags": ["pgm"],
    "learn_time": 1.6859122565541744,
    "summary": "Loopy belief propagation sounds like a hack, but it can be interpreted as a variational inference algorithm. In particular, it is a fixed point update for an approximation to variational inference, where both the energy functional and the marginal polytope are approximated. While this analysis doesn't lead to any strong guarantees, it is the basis for generalizations of loopy BP which have stronger guarantees.\n",
    "last_mod": "2014-08-13T09:59:02.294Z",
    "tag": "loopy_bp_as_variational",
    "exercises": null,
    "is_shortcut": false,
    "software": null
  },
  "model": "graph.concept",
  "pk": "886laqe9"
}
